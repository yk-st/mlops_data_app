{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仮説設定をしてKPIを定めてみよう\n",
    "\n",
    "データとして出てくるのが以下のようなデータです。\n",
    "\n",
    "- check：決済した\n",
    "- id: ユーザID\n",
    "- money:いくら使った\n",
    "\n",
    "```\n",
    "+-------+---+-----+\n",
    "|actions| id|money|\n",
    "+-------+---+-----+\n",
    "|  check|  2|  200|\n",
    "|  check|  1| 1000|\n",
    "+-------+---+-----+\n",
    "\n",
    "```\n",
    "\n",
    "例えば、\n",
    "現状は、お金を使ってくれるユーザの売上（実際だと直近1週間などで計算することが多い）が5000円以上のユーザをロイヤルユーザとして  \n",
    "そこに分類する人たちがより多くのおかねを使ってくれるのではないか？といった施作が考えられる。\n",
    "\n",
    "そこで、お金をたくさん使ってくれるユーザ(ロイヤルユーザ)に対して、Aの広告を出力する  \n",
    "お金をあまり使ってくれないユーザに対して、Bの広告を出力する\n",
    "クーポンを配布するなどでもOK。\n",
    "\n",
    "今回は、簡単のために\n",
    "モデルの成功としては、現状の全体の売上1200円(KPI)でそれが１０％上がれば成功といえる(ことにする)。\n",
    "（経費は分かりやすさのため0円）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回の環境についての説明 \n",
    "環境説明のためのコマンド群\n",
    "\n",
    "## 環境の立ち上げ\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "## dockerコンテナへログイン\n",
    "\n",
    "```\n",
    "docker exec -it pyspark_mlops /bin/bash\n",
    "```\n",
    "\n",
    "## pyspark-topicの作成コマンド\n",
    "\n",
    "```\n",
    "/home/pyspark/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic pyspark-topic \\\n",
    "    --replication-factor 1 \\\n",
    "    --partitions 1 \\\n",
    "    --bootstrap-server kafka_mlops:9092 \n",
    "```\n",
    "\n",
    "## Pyspakrでストリーミングデータを読み取る\n",
    "\n",
    "### 接続のためのコマンド\n",
    "\n",
    "```\n",
    "pyspark --packages org.apache.spark:spark-streaming_2.13:3.2.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.2,org.apache.spark:spark-avro_2.12:3.2.2\n",
    "```\n",
    "\n",
    "### ストリーミングの準備\n",
    "\n",
    "```\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka_mlops:9092\") \\\n",
    "  .option(\"subscribe\", \"pyspark-topic\") \\\n",
    "  .load()\n",
    "```\n",
    "\n",
    "### ストリーミングデータの読み込み\n",
    "\n",
    "```\n",
    "file_stream = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", \"/tmp/share_file/datalake/web_actions/\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .partitionBy(\"key\") \\\n",
    "  .trigger(processingTime=\"5 seconds\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kafka/parquet/\") \\\n",
    "  .start()\n",
    "```\n",
    "\n",
    "### 停止するときはこちら\n",
    "\n",
    "```\n",
    "file_stream.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの取得と蓄積\n",
    "末尾のIDやURLを変えながら、何回かデータを送ってみます。  \n",
    "IDは最低でも3人以上用意してみましょう。\n",
    "\n",
    "### 買い上げ完了画面\n",
    "http://localhost:3001/done/?id=1\n",
    "\n",
    "## データの読み込みと書き込みでデータウェアハウスにデータを保存してきましょう\n",
    "\n",
    "```\n",
    "df=spark.read.parquet(\"/tmp/share_file/datalake/web_actions\")\n",
    "```\n",
    "\n",
    "### jsonをバラバラにして扱いやすくする\n",
    "\n",
    "```\n",
    "df.createOrReplaceTempView(\"web_actions\")\n",
    "result_df=spark.sql(\"select key,id,money,actions,sendtime from web_actions LATERAL VIEW json_tuple(value,'id','money','sendtime', 'actions') user as id, money, actions, sendtime \")\n",
    "```\n",
    "\n",
    "### ファイルをCSVで吐き出す\n",
    "\n",
    "```\n",
    "result_df.coalesce(1).write.mode('overwrite').parquet(\"/tmp/share_file/datamart/web_actions/\")\n",
    "```\n",
    "\n",
    "# データを読み込んでみる\n",
    "\n",
    "```\n",
    "df2 = spark.read.parquet(\"/tmp/share_file/datamart/web_actions/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのテストとSparkの使い方速習\n",
    "\n",
    "データのテストは、自分自身が思い描いた形にデータがなっているかを確認すること。  \n",
    "データは多くの人に使われることを想定するため、汎用的な状態で保存されていることが多い。  \n",
    "自身が実現したいデータの状態に変更していくことが大事。\n",
    "\n",
    "また、思いがけず変なデータ（例えば、売上のデータなのになぜかマイナスになっているなど）があることがあるのでそのようなデータは除外したりすることで\n",
    "機械学習のモデルの担保を行うことが多い。\n",
    "\n",
    "## データインテグレティ\n",
    "データのテストをする際に確認すると良いポイントとしてデータインテグレティが存在する。  \n",
    "いくつか指標があるが、重要なポイントをいくつかピックアップ。\n",
    "\n",
    "- 完全性 -> 活動に関する全ての情報が記録されているか？\n",
    "- 判読性 -> 人間が理解できる？プログラムとして定義できる？\n",
    "\n",
    "これらを確認するチェックを行うと良い。\n",
    "\n",
    "実はデータを活用するプロジェクトでは、80%以上の時間をこのテストをする時間に割いていると言われています。\n",
    "\n",
    "### 完全性のチェックの例\n",
    "データとして本来であればlogin -> add_cart -> checkの３種類が必ず存在しているはず。  \n",
    "仮にloginがなくてadd_cartとcheckだけであればなにか不正なデータの可能性やシステムの不具合などの状態が考えらるため  \n",
    "計算から除外するなどの対応を検討しないといけない。\n",
    "\n",
    "```\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df=df.groupby('id').agg(F.collect_list('actions').alias('arrayed'))\n",
    "df.withColumn('check_completion', F.when(F.array_contains(F.col('arrayed'),'check'),F.array_contains(F.col('arrayed'),'login'))).show()\n",
    "```\n",
    "\n",
    "### 判読性のチェックの例\n",
    "例えば、売上のデータにマイナスがなどがあると人間が理解できない状態になる。  \n",
    "「なんで、マイナス？」「売上といいつつ、払い戻しもある？」などの考えが巡ってしまう。  \n",
    "データを正しく使うためにデータを正しく理解する必要がある。\n",
    "\n",
    "sparkプログラム\n",
    "\n",
    "```\n",
    "df.withColumn(\"check_money\", F.col(\"money\") > 0).groupby(\"check_money\").count().show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMLで簡単なモデルを作成してみましょう\n",
    "\n",
    "今回はKmeansを使って、データを2つに分類していきます。\n",
    "\n",
    "分類に利用するデータは、売上で売上が高い人と低い人で分類を行なっていきます。\n",
    "\n",
    "lectureフォルダに配置された「KMeans.py」をみながら話を進めていきましょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの結果をデータベースに保存してみよう\n",
    "\n",
    "いよいよモデルの結果をデプロイしていきます。  \n",
    "今回はモデルの結果をmongodbに格納してNodeJs上のアプリケーションから利用していきます。\n",
    "\n",
    "## mongodbへの保存を行なっていきます\n",
    "\n",
    "```\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.2 \\\n",
    "        --conf spark.mongodb.input.uri=mongodb://action:pass123@mongo_data_mlops:27017/user_prediction \\\n",
    "        --conf spark.mongodb.output.uri=mongodb://action:pass123@mongo_data_mlops:27017/user_prediction\n",
    "```\n",
    "\n",
    "## parquetの読み込み\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"/tmp/share_file/datamodel/part1\")\n",
    "```\n",
    "\n",
    "## mongodbへの書き込みを行う\n",
    "\n",
    "```\n",
    "df.repartition(1).write \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option( \"uri\", \"mongodb://action:pass123@mongo_data_mlops:27017/user_prediction.prediction\") \\\n",
    "    .save()\n",
    "```\n",
    "\n",
    "## mongodbのデータを確認してみよう\n",
    "書き込んだデータを確認してみましょう。\n",
    "\n",
    "### 対象のコンテナに接続\n",
    "\n",
    "```\n",
    "docker exec -it mongo_data_mlops /bin/bash\n",
    "```\n",
    "\n",
    "### mongodbに接続を行う\n",
    "mongo -u action -p pass123 user_prediction\n",
    "\n",
    "### 簡単にいくつか検索してみます\n",
    "predictionはテーブル名\n",
    "\n",
    "```\n",
    "db.prediction.find()\n",
    "db.prediction.find({id:1})\n",
    "db.prediction.find({id:1},{predictions:1, _id:0})\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
