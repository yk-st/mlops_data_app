{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの出力結果(mongodb)とデータアプリケーション(Nodejs)を連携してみよう\n",
    "\n",
    "今回はモデルの結果はmongodbに格納済み  \n",
    "格納したデータをデータアプリケーションであるWebアプリから取得していきます。\n",
    "\n",
    "「express.js」をもとに話を進めていきましょう。\n",
    "\n",
    "## 通常の決済画面も何回かデータを送っておきましょう\n",
    "\n",
    "### 買い上げ完了画面\n",
    "http://localhost:3001/done/?id=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 施作効果を確認してみよう\n",
    "\n",
    "ここまでで、MlOpsの単純な流れは一度みたことになります。  \n",
    "いくつか、初期の段階よりもデータが増えているはずです、まずはデータの確認から始めましょう\n",
    "\n",
    "## データの確認\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"/tmp/share_file/datalake/web_actions/\")\n",
    "```\n",
    "\n",
    "ad_throuth_Aとad_throuth_Bが増えてます。\n",
    "\n",
    "こちらもデータの取得と蓄積作業の一部で、  \n",
    "このようにしておくと施策の効果としてad_throuth_Aでの売上がいくらとか、ad_throuth_Bでの売上がいくら\n",
    "となって他のキャンペーンとかに邪魔されずデータを絞り込んだり、取得することができるようになってきます。\n",
    "\n",
    "## groupbyでそれぞれのアクションごとの売り上げを見てみましょう\n",
    "\n",
    "```\n",
    "import pyspark.sql.functions as F\n",
    "df3 = df2.withColumn(\"money\", F.col(\"money\").cast(\"integer\")).groupby(\"action\", \"id\").sum(\"money\")\n",
    "```\n",
    "\n",
    "果たして、全体の売上が10%上がったでしょうか？それとも？\n",
    "1200円\n",
    "\n",
    "```\n",
    "df2.withColumn(\"money\", F.col(\"money\").cast(\"integer\")).select(F.sum(\"money\")).show()\n",
    "df2.withColumn(\"money\", F.col(\"money\").cast(\"integer\")).select(F.sum(\"money\").alias(\"allmoney\")).withColumn(\"kpi\", F.col(\"allmoney\") / F.lit(\"1200\") ).show()\n",
    "```\n",
    "\n",
    "## おまけ　時間で絞るとより正確に\n",
    "\n",
    "```\n",
    "df3 = df2.filter(F.col(\"sendtime\") > F.unix_timestamp(F.lit(\"2022-08-23 13:40:48\"))).withColumn(\"money\", F.col(\"money\").cast(\"integer\")).groupby(\"action\", \"id\").sum(\"money\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの再学習を検討しよう\n",
    "\n",
    "モデルは一度学習しただけで完了ではありません。\n",
    "モニタリングや結果を確認した上で、思った通りの結果が出なかったり、最初は調子良かったが、外部の影響などで途中から悪くなったりします。\n",
    "\n",
    "売上が思った以上に伸びなかった。さらに伸ばしたいという際は再度学習することも考慮に入れます。\n",
    "\n",
    "指標は様々ですが、\n",
    "\n",
    "- KPIとして定めた指標が悪い方向に変動を始めたらモデルの再学習を検討し始める\n",
    "- 毎日定期的にモデルを更新する\n",
    "\n",
    "今回は、KMeans.pyを使って再学習をおこなってみましょう。\n",
    "\n",
    "## Kmeans.pyを使って再学習\n",
    "データの保存先はpart2に保存するようにしてみましょう"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルを継続的にデプロイしよう\n",
    "\n",
    "モデルの学習が完了したら\n",
    "\n",
    "## mongodbへの保存を行なっていきます\n",
    "\n",
    "```\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.3 \\\n",
    "        --conf spark.mongodb.input.uri=mongodb://action:pass123@mongo_data_mlops:27017/user_prediction \\\n",
    "        --conf spark.mongodb.output.uri=mongodb://action:pass123@mongo_data_mlops:27017/user_prediction\n",
    "```\n",
    "\n",
    "## parquetの読み込み\n",
    "\n",
    "```\n",
    "df = spark.read.parquet(\"/tmp/share_file/datamodel/part2\")\n",
    "```\n",
    "\n",
    "## mongodbへの書き込みを行う\n",
    "\n",
    "```\n",
    "df.repartition(1).write.mode('overwrite') \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option( \"uri\", \"mongodb://action:pass123@mongo_data_mlops:27017/user_prediction.prediction\") \\\n",
    "    .save()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
